{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import WeRateDogs Twitter archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WRD_twitter = pd.read_csv('twitter-archive-enhanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import image prediction file from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.codementor.io/aviaryan/downloading-files-from-urls-in-python-77q3bs0un\n",
    "\n",
    "import requests\n",
    "import os\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "r = requests.get(url)\n",
    "open('image-predictions.tsv', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img_pred = pd.read_csv('image-predictions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Tweet JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: Dhaval P's answer on his question: https://knowledge.udacity.com/questions/47704\n",
    "import json\n",
    "data = []\n",
    "with open('tweet_json.txt') as f:    \n",
    "        for line in f:         \n",
    "            data.append(json.loads(line))\n",
    "df_twit_JSON = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Assess Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing WeRateDogs Twitter Archive Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issues:\n",
    "- Of the 2356 entries, there are only approximately 400 which have a declared dog type (i.e. doggo, puppo, etc.). This is either because there are not enough established variables for the wide variety of dog categories, the majority of tweets do not implement use of dog categories, or the dataset did not extract all the category mentions from the tweets.\n",
    "- Dog names ('name') has 745 extracted as a non-null 'None', and several dog names extracted as 'a', 'the', and 'an'. Most of the Nones are appropriate, and most of the 'a', 'the', and 'an' entries should also be changed to 'None'.\n",
    "- Entry at index 2204 has to be renamed to 'Berta'\n",
    "- There are 181 retweet entries, and the project dictates only having original tweets. Should be removed.\n",
    "- There are 78 reply tweet entries, and I'm not sure if that fits into the definition of 'originial tweet' even if it includes new photo, name and rating. Better to err on the side of caution and remove them.\n",
    "- Entry at index 313 extracted a rating of '960/0', and needs to be changed to the revised rating of '13/10'\n",
    "- Entries at index 340 and 695 extracted a rating of '75/10', and needs to be changed to the actual rating of '9.75/10'\n",
    "- Entry at index 342 actually doesn't have a rating ('11/15' was extracted, while it was simply a description of time). Row needs to be removed.\n",
    "- Entry at index 516 actually doesn't have a rating ('24/7' was extracted, while it was simply a description of time). Row needs to be removed.\n",
    "- Entry at index 763 extracted a rating of '27/10', and needs to be changed to the actual rating of '11.27/10'\n",
    "- Entry at index 1068 extracted a rating of '9/11', and needs to be changed to the actual rating of '14/10'\n",
    "- Entry at index 1165 extracted a rating of '4/20', and needs to be changed to the actual rating of '13/10'\n",
    "- Entry at index 1202 extracted a rating of '50/50', and needs to be changed to the actual rating of '11/10'\n",
    "- Entries at indices 1598 and 1663 were technically not officially given ratings by WeRateDogs, and should be removed.\n",
    "- Entry at index 1662 extracted a rating of '7/11', and needs to be changed to the actual rating of '10/10'\n",
    "- Entry at index 1712 extracted a rating of '26/10', and needs to be changed to the actual rating of '11.26/10'\n",
    "- Entry at index 2335 extracted a rating of '1/2', and needs to be changed to the actual rating of '9/10'\n",
    "- Since some correct ratings contain decimal values, 'rating_numerator' and 'rating_denominator' need to be changed from int to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidiness Issues:\n",
    "- Dog types (i.e. doggo, puppo, etc.) are in separate variable columns, where if a dog is described as such, the value is the dogtype, whereas if it isn't, the value is a non-null 'None'. Instead the columns could either be framed as Boolean 1's and 0's, or all placed into one 'dog_type' variable column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WRD_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WRD_twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: https://stackoverflow.com/questions/33042633/selecting-last-n-columns-and-excluding-last-n-columns-in-dataframe\n",
    "dog_type_cols = df_WRD_twitter.columns[-5:].values\n",
    "\n",
    "for i in dog_type_cols:\n",
    "    print(df_WRD_twitter[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/25351968/how-to-display-full-non-truncated-dataframe-information-in-html-when-convertin/25352191\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "df_WRD_twitter[df_WRD_twitter['name']=='None']['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WRD_twitter[df_WRD_twitter['name']=='a']['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WRD_twitter[df_WRD_twitter['name']=='an']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WRD_twitter[df_WRD_twitter['name']=='the']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WRD_twitter.rating_numerator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WRD_twitter[df_WRD_twitter['rating_numerator'] >= 20][['rating_numerator','rating_denominator','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WRD_twitter.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WRD_twitter[df_WRD_twitter['rating_denominator'] != 10][['rating_numerator','rating_denominator','text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Image Prediction Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issues:\n",
    "- There are 324 images that returned predictions that were not dogs ('p1_dog', 'p2_dog', 'p3_dog' all False). These rows are either evidence of the neural network discovering pictures that indeed don't contain dogs, or of the neural network doing a poor job of finding the dog in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img_pred.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img_pred.img_num.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img_pred.tweet_id.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pict_false = df_img_pred[(df_img_pred.p1_dog == False) & (df_img_pred.p2_dog == False) & (df_img_pred.p3_dog == False)]\n",
    "all_pict_false.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Twitter JSON Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issues:\n",
    "- There are 179 tweets that are retweets. These should be removed, as they are not originals.\n",
    "- There are 28-29 tweets that are original responses to other tweets. As they are not necessarily stand-alone originals, so may be up for removal, unless the image dataset has extracted photos associated with the tweet.\n",
    "- Essentially empty columns that should be dropped or ignored from merging: 'contributors', 'coordinates', 'geo', 'place'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidiness Issues:\n",
    "- Columns in which entries contain multiple pieces of information: 'entities', 'extended_entities', 'quoted_status', 'retweeted_status', 'user'. These columns could be made into their own datasets, or their contents could be sorted into unique variables that would be attached onto the end of the main JSON dataset entries to which they belong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twit_JSON.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twit_JSON.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df_twit_JSON[df_twit_JSON.retweeted_status.isna() != True]['retweeted_status'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twit_JSON[df_twit_JSON.retweeted_status.isna() != True].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twit_JSON[df_twit_JSON.quoted_status.isna() != True].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twit_JSON.favorited.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Quality Issues:\n",
    "\n",
    "#### WeRateDogs Twitter Archive Data:\n",
    "- Of the 2356 entries, there are only approximately 400 which have a declared dog type (i.e. doggo, puppo, etc.). This is either because there are not enough established variables for the wide variety of dog categories, the majority of tweets do not implement use of dog categories, or the dataset did not extract all the category mentions from the tweets.\n",
    "- Dog names ('name') has 745 extracted as a non-null 'None', and several dog names extracted as 'a', 'the', and 'an'. Most of the Nones are appropriate, and most of the 'a', 'the', and 'an' entries should also be changed to 'None'.\n",
    "- Entry at index 2204 has to be renamed to 'Berta'\n",
    "- There are 181 retweet entries, and the project dictates only having original tweets. Should be removed.\n",
    "- There are 78 reply tweet entries, and I'm not sure if that fits into the definition of 'originial tweet' even if it includes new photo, name and rating. Better to err on the side of caution and remove them.\n",
    "- Entry at index 313 extracted a rating of '960/0', and needs to be changed to the revised rating of '13/10'\n",
    "- Entries at index 340 and 695 extracted a rating of '75/10', and needs to be changed to the actual rating of '9.75/10'\n",
    "- Entry at index 342 actually doesn't have a rating ('11/15' was extracted, while it was simply a description of time). Row needs to be removed.\n",
    "- Entry at index 516 actually doesn't have a rating ('24/7' was extracted, while it was simply a description of time). Row needs to be removed.\n",
    "- Entry at index 763 extracted a rating of '27/10', and needs to be changed to the actual rating of '11.27/10'\n",
    "- Entry at index 1068 extracted a rating of '9/11', and needs to be changed to the actual rating of '14/10'\n",
    "- Entry at index 1165 extracted a rating of '4/20', and needs to be changed to the actual rating of '13/10'\n",
    "- Entry at index 1202 extracted a rating of '50/50', and needs to be changed to the actual rating of '11/10'\n",
    "- Entries at indices 1598 and 1663 were technically not officially given ratings by WeRateDogs, and should be removed.\n",
    "- Entry at index 1662 extracted a rating of '7/11', and needs to be changed to the actual rating of '10/10'\n",
    "- Entry at index 1712 extracted a rating of '26/10', and needs to be changed to the actual rating of '11.26/10'\n",
    "- Entry at index 2335 extracted a rating of '1/2', and needs to be changed to the actual rating of '9/10'\n",
    "- Since some correct ratings contain decimal values, 'rating_numerator' and 'rating_denominator' need to be changed from int to float\n",
    "\n",
    "#### Image Prediction Data:\n",
    "- There are 324 images that returned predictions that were not dogs ('p1_dog', 'p2_dog', 'p3_dog' all False). These rows are either evidence of the neural network discovering pictures that indeed don't contain dogs, or of the neural network doing a poor job of finding the dog in the image.\n",
    "\n",
    "#### Twitter JSON Data:\n",
    "- There are 179 tweets that are retweets. These should be removed, as they are not originals.\n",
    "- There are 28-29 tweets that are original responses to other tweets. As they are not necessarily stand-alone originals, so may be up for removal, unless the image dataset has extracted photos associated with the tweet.\n",
    "- Essentially empty columns that should be dropped or ignored from merging: 'contributors', 'coordinates', 'geo', 'place'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Tidiness Issues:\n",
    "\n",
    "#### WeRateDogs Twitter Archive Data:\n",
    "- Dog types (i.e. doggo, puppo, etc.) are in separate variable columns, where if a dog is described as such, the value is the dogtype, whereas if it isn't, the value is a non-null 'None'. Instead the columns could either be framed as Boolean 1's and 0's, or all placed into one 'dog_type' variable column.\n",
    "\n",
    "#### Twitter JSON Data:\n",
    "- Columns in which entries contain multiple pieces of information: 'entities', 'extended_entities', 'quoted_status', 'retweeted_status', 'user'. These columns could be made into their own datasets, or their contents could be sorted into unique variables that would be attached onto the end of the main JSON dataset entries to which they belong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Example         Dataset      Var Name   Var Type\n",
    "666020888022790149 Image data: 'tweet_id' type int64\n",
    "\n",
    "892420643555336193 WRD data:   'tweet_id' type int64\n",
    "\n",
    "892420643555336193 JSON data:  'id'       type int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twit_JSON.drop(['contributors', \n",
    "                   'coordinates', \n",
    "                   'geo',\n",
    "                   'id',\n",
    "                   'in_reply_to_screen_name', \n",
    "                   'in_reply_to_status_id', \n",
    "                   'in_reply_to_status_id_str',\n",
    "                   'in_reply_to_user_id',\n",
    "                   'in_reply_to_user_id_str',\n",
    "                   'place',\n",
    "                   'quoted_status',\n",
    "                   'quoted_status_id',\n",
    "                   'quoted_status_id_str',\n",
    "                   \n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
